---
name: thinking-fast-slow
description: Kahneman's discipline for decisions and forecasts. Activate when confidence is high, when numbers are involved, when the room agrees too quickly, or when you're about to predict anything. The more certain you feel, the more you need this.
allowed-tools: Read, Edit, AskUserQuestion
---

# Thinking, Fast and Slow

Your brain has two modes. System 1 is fast, automatic, confident. System 2 is slow, effortful, lazy. You think you're using System 2. You're almost always on System 1.

This matters because System 1 generates certainty based on story coherence, not evidence quality. Compelling narratives feel true. Missing information isn't flagged. The less you know, the easier everything fits.

Knowing this won't fix you. But it lets you design around it.

---

## When to Use This

**Before making a forecast:**
- Estimating timelines, costs, or outcomes
- Presenting projections to stakeholders
- Deciding whether a project is feasible
- "I'm 90% confident we can ship by March"

**Before making a decision:**
- High-stakes choice with incomplete information
- The team is converging quickly on an option
- Someone just threw out a number (you're now anchored)
- You feel very sure under time pressure

**When auditing reasoning:**
- Reviewing someone's plan or projection
- Checking your own logic before presenting
- The room agreed too fast
- Something feels too clean

**The rule:** If confidence is high and stakes are real, apply this. Especially your own confidence.

---

## The Core Problem

Confidence is not a signal of accuracy. This is hard to accept because confidence feels like information.

System 1 builds coherent stories and tags them with certainty. That feeling of "this is obviously right" comes from narrative fit, not evidence. You don't notice what's missing because the story works without it.

The articulate, confident person in the room is not your best source of truth. Including—especially—yourself. Kahneman spent his career studying these errors. He still makes them.

---

## The Principles

These apply to all modes. They're about the gap between feeling right and being right.

### WYSIATI: What You See Is All There Is

Your brain builds the best story from available information—then acts as if that's all the information. Missing data doesn't register as uncertainty. It just doesn't exist.

The cleaner the story, the more suspicious you should be. Reality is messier than convincing narratives.

### Substitution

Hard questions get swapped for easy ones without you noticing.

"How likely is this to succeed?" becomes "How easily can I imagine success?"
"Is this person competent?" becomes "Do I like this person?"
"What's the probability?" becomes "How vivid is the example?"

You answer the substituted question confidently. The hard question goes unasked.

### Regression to the mean

Extreme performance is partly skill, partly luck. The luck portion won't repeat. But you'll attribute the whole thing to skill, then be surprised when performance "reverts."

This isn't pessimism. It's statistics. The brilliant quarter was partly noise. The disaster quarter was partly noise too.

### What confidence actually signals

Expert intuition is real—when:
- The environment is regular and predictable
- You've had extensive practice with immediate, clear feedback
- You've made this exact decision type hundreds of times

Firefighter sensing danger: trust it. CEO sensing market shift: skeptical. VC sensing which startup wins: basically noise with conviction.

The test: Has this person made this specific decision type many times, with fast accurate feedback? If not, their confidence is just confidence.

---

## When Activated

Determine the mode from context or by asking:

- **Forecast mode** — "I need to estimate, predict, or project something"
- **Decision mode** — "I'm about to choose, and the stakes matter"
- **Audit mode** — "I want to check reasoning for bias patterns"

---

## Forecast Mode: Before You Predict

You're about to estimate a timeline, project an outcome, or claim a probability. Your inside view is vivid and optimistic. Let's introduce some friction.

### The reference class

Don't ask "how long will this take?" Ask: "how long do projects like this typically take?"

Your inside view knows your special circumstances—your smart team, your unique approach, your particular advantages. Every failed project had those too.

**The questions:**
1. What's the reference class? (Projects of this type, not "projects I've worked on")
2. What's the base rate for that class? (How long do they actually take? What's the success rate?)
3. Why would you be different? (Be specific. "We're good" isn't a reason.)

Start from the outside view. Adjust toward your inside view only for concrete, articulable reasons.

### The premortem

It's a year from now. The project failed—badly. Not a little behind, not a modest miss. A real failure.

What happened?

This isn't pessimism. It's legitimized doubt. "What could go wrong?" invites dismissal. "It did go wrong—explain why" surfaces risks that optimism suppresses.

Run this before the decision locks. After commitment, motivated reasoning defends the choice.

### Widen the intervals

If you think 3-6 months, it's probably 5-12. Professional forecasters underestimate uncertainty by 2-3x.

**The discipline:**
- State your intuitive range
- Widen it by 50% on each end
- Ask: "Would I bet real money at these boundaries?"
- If you hesitate, widen again

Overconfidence in ranges is one of the most robust findings in judgment research. You are not the exception.

### Output for Forecast Mode

1. **The prediction**: State it clearly
2. **Reference class**: What base rate applies?
3. **Inside view adjustments**: Specific reasons you'd differ
4. **Premortem risks**: Top 3 failure modes
5. **Widened range**: The honest interval

---

## Decision Mode: Before You Choose

You're about to make a call. The stakes are real. You feel reasonably confident. Let's check.

### Check for anchoring

If you heard a number before forming your own view, you're compromised. The first number shapes everything after it—even when you know it's arbitrary, even when you try to ignore it.

**The question:** What would I estimate if I'd never heard that number?

Write down your independent estimate before discussion. After you hear others' numbers, it's too late.

### Seek disconfirmation

What would have to be true for this to be wrong?

If you can't answer, you don't understand your own reasoning. You've built a story, not an argument.

**The discipline:**
1. State your position clearly
2. Name the strongest version of the opposing view
3. Identify what evidence would change your mind
4. Go look for that evidence

Confirmation is automatic. Disconfirmation is work. That's why it's valuable.

### Check for WYSIATI

What information is missing? What didn't you look for because the answer already felt complete?

The danger isn't the information you know is missing. It's the information you don't realize you're lacking because your story doesn't need it.

**The questions:**
1. What do we know? (Actually know—evidence, not inference)
2. What are we assuming? (What feels true but hasn't been verified?)
3. What would a skeptic ask for?

### Separate evidence from story

How many different stories could explain the same data? You're attached to the most convenient one.

Two data points don't make a pattern. Three quotes don't make a trend. Your brain sees signal in noise because signal is more satisfying.

### Output for Decision Mode

1. **The decision**: What you're leaning toward
2. **Anchor check**: Were you exposed to a number? What's your independent estimate?
3. **Disconfirmation**: What would prove you wrong? Did you look?
4. **Missing information**: What don't you know that you should?
5. **Alternative stories**: What else could explain the same evidence?

---

## Audit Mode: Checking Reasoning

You're reviewing a plan, a projection, someone's logic—possibly your own. The goal is pattern-matching for bias, not proving wrongness.

### The scan

**For forecasts, check:**
- Is there a reference class? Or only inside view?
- How wide are the confidence intervals? (Narrow = suspicious)
- Did they run a premortem?
- What's their track record on similar predictions?

**For decisions, check:**
- Was there an anchor? (First number in the room?)
- How quickly did consensus form? (Fast = groupthink)
- What disconfirming evidence was sought?
- What information is missing from the story?

**For confidence, check:**
- Is this a domain where expert intuition works? (Regular environment, fast feedback, many repetitions?)
- Is confidence based on story coherence or evidence quality?
- What would they need to see to change their mind?

### The questions to ask

These aren't accusations. They're structured doubt.

- "What's the base rate for projects like this?"
- "What would have to be true for this to be wrong?"
- "What information are we missing?"
- "Let's run a quick premortem—it's a year later and this failed. What happened?"
- "If we hadn't heard [that number], what would we estimate?"

### Managing the room

**Before preferences emerge:** Collect independent estimates in writing. After discussion starts, anchoring and groupthink take over.

**Name the bias, not the person:** "I notice we might be anchored on that first number—let's try regenerating without it" works. "You're anchored" is a fight.

**Criteria before options:** Agree on what matters before seeing choices. After, you'll weight criteria to justify your favorite.

### Output for Audit Mode

1. **What's the reasoning?** Summarize the logic being checked
2. **Bias patterns detected:** Name them specifically
3. **Missing elements:** Reference class? Disconfirmation? Premortem?
4. **Suggested friction:** Questions or processes to add
5. **The honest take:** Is this reasoning or rationalization?

---

## Loss Aversion, Briefly

Losses hurt roughly twice as much as equivalent gains feel good. This makes you:
- Risk-averse when protecting gains
- Risk-seeking when facing losses (desperate gambles to avoid locking in a loss)
- Overweight small probabilities when stakes feel large

Whether something reads as gain or loss is malleable. Frame choices accordingly—for yourself and others.

---

## What This Won't Fix

Knowing about biases doesn't make you immune. Kahneman wrote the book on these errors. He still makes them. So will you.

The biases run on automatic processes that don't listen to lectures. You'll still feel confident when you shouldn't. You'll still anchor on the first number. You'll still build convincing stories from scraps.

What helps isn't awareness—it's structure:
- Decision processes that force slow thinking at key moments
- Checklists requiring specific questions before you can proceed
- Outside views from people not anchored to your framing
- Assuming you're overconfident and widening all estimates by default

You won't debias yourself. You might debias your decisions.

---

## Quick Reference

### Red flags in your own thinking

- High confidence under time pressure
- Story feels clean and complete
- You can't name what would change your mind
- Someone threw out a number before you formed a view
- It's your project (ownership biases toward optimism)

### Red flags in others' reasoning

- Narrow confidence intervals
- No reference class, only inside view
- Quick consensus
- Detailed narrative, thin evidence
- Can't articulate disconfirming case

### The fundamental discipline

Intelligence doesn't help. Smart people are worse—better at constructing convincing stories, more confidence in them.

The discipline is checking when you feel most sure. Running the premortem when you're excited. Finding the base rate when your inside view is vivid. Widening the interval when you'd bet your reputation.

Your brain will call this overthinking. It's not. Slowing down costs minutes. Confident speed costs projects.
